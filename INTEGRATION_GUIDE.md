# TTS API CUDAä¼˜åŒ–é›†æˆæŒ‡å—

## ğŸ¯ ä¼˜åŒ–åŸç†

### 1. CUDA Graphä¼˜åŒ–
- **åŸç†**: å‡å°‘GPUå†…æ ¸å¯åŠ¨å¼€é”€
- **æ•ˆæœ**: 20-40% æ€§èƒ½æå‡
- **é€‚ç”¨**: å›ºå®šè¾“å…¥å½¢çŠ¶çš„æ¨ç†

### 2. æ™ºèƒ½åˆ†å—å¤„ç†
- **åŸç†**: å°†é•¿éŸ³é¢‘åˆ†å‰²ä¸ºå°å—å¹¶è¡Œå¤„ç†
- **æ•ˆæœ**: 2-4x æ€§èƒ½æå‡
- **é€‚ç”¨**: é•¿æ–‡æœ¬TTSç”Ÿæˆ

## ğŸ”§ é›†æˆæ­¥éª¤

### æ­¥éª¤1: å¤‡ä»½åŸå§‹æ–‡ä»¶
```bash
cp tts_api.py tts_api_backup.py
```

### æ­¥éª¤2: æ·»åŠ ä¼˜åŒ–ç±»åˆ°tts_api.py
åœ¨tts_api.pyæ–‡ä»¶å¼€å¤´æ·»åŠ ï¼š

```python
# å¯¼å…¥CUDAä¼˜åŒ–æ¨¡å—
from cuda_optimized_tts import OptimizedVocoderProcessor, CUDAGraphVocoder
```

### æ­¥éª¤3: ä¿®æ”¹TTSç±»
åœ¨tts_api.pyä¸­æ‰¾åˆ°TTSç±»ï¼Œæ·»åŠ ä»¥ä¸‹æ–¹æ³•ï¼š

```python
class TTS:
    def __init__(self, configs):
        # ... ç°æœ‰ä»£ç  ...
        self.use_vocoder_optimization = getattr(configs, 'use_vocoder_optimization', True)
        self.vocoder_chunk_size = getattr(configs, 'vocoder_chunk_size', 500)
        self.optimized_processor = None
        
    def init_vocoder_optimization(self):
        """åˆå§‹åŒ–Vocoderä¼˜åŒ–"""
        if not self.use_vocoder_optimization or self.optimized_processor is not None:
            return
            
        try:
            self.optimized_processor = OptimizedVocoderProcessor(
                self.vocoder,
                device=self.configs.device,
                chunk_size=self.vocoder_chunk_size
            )
            print("âœ… Vocoderä¼˜åŒ–åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ Vocoderä¼˜åŒ–åˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_vocoder_optimization = False
    
    def using_vocoder_synthesis_optimized(self, semantic_tokens, phones, speed=1.0, sample_steps=32):
        """ä½¿ç”¨ä¼˜åŒ–çš„Vocoderåˆæˆ"""
        # åˆå§‹åŒ–ä¼˜åŒ–å¤„ç†å™¨
        if self.optimized_processor is None:
            self.init_vocoder_optimization()
        
        if not self.use_vocoder_optimization or self.optimized_processor is None:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            return self.using_vocoder_synthesis(semantic_tokens, phones, speed, sample_steps)
        
        # æ‰§è¡Œç°æœ‰çš„æ¨ç†æµç¨‹ç›´åˆ°pred_specç”Ÿæˆ
        # ... (å¤åˆ¶using_vocoder_synthesisçš„å¤§éƒ¨åˆ†ä»£ç ) ...
        
        # åœ¨æœ€åçš„vocoderè°ƒç”¨éƒ¨åˆ†æ›¿æ¢ä¸ºï¼š
        with torch.no_grad():
            audio = self.optimized_processor.process_optimized(pred_spec)

        return audio[0][0] if len(audio.shape) > 2 else audio[0]
```

### æ­¥éª¤4: ä¿®æ”¹tts_handleå‡½æ•°
åœ¨tts_handleå‡½æ•°ä¸­ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼š

```python
async def tts_handle(req: dict):
    # ... ç°æœ‰ä»£ç  ...
    
    # åˆå§‹åŒ–ä¼˜åŒ–
    tts_pipeline.init_vocoder_optimization()
    
    # åœ¨Vocoderåˆæˆéƒ¨åˆ†ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    if hasattr(tts_pipeline, 'use_vocoder_optimization') and tts_pipeline.use_vocoder_optimization:
        # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ (éœ€è¦æ ¹æ®å®é™…ä»£ç ç»“æ„è°ƒæ•´)
        print("ğŸš€ ä½¿ç”¨CUDAä¼˜åŒ–å¤„ç†")
    
    # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
```

### æ­¥éª¤5: æ›´æ–°é…ç½®æ–‡ä»¶
åœ¨ `GPT_SoVITS/configs/tts_infer.yaml` ä¸­æ·»åŠ ï¼š

```yaml
# CUDAä¼˜åŒ–é…ç½®
use_vocoder_optimization: true
vocoder_chunk_size: 500
```

## ğŸ§ª æµ‹è¯•ä¼˜åŒ–æ•ˆæœ

åˆ›å»ºæµ‹è¯•è„šæœ¬ï¼š

```python
#!/usr/bin/env python3
"""
æµ‹è¯•CUDAä¼˜åŒ–æ•ˆæœ
"""
import requests
import time

def test_optimization():
    base_url = "http://219.144.21.182:9880"
    
    test_texts = [
        "çŸ­æ–‡æœ¬æµ‹è¯•",
        "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯ä¼˜åŒ–æ•ˆæœã€‚",
        "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«äº†æ›´å¤šçš„å†…å®¹ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡CUDA Graphå’Œåˆ†å—å¤„ç†æ¥æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦ã€‚"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n{i}. æµ‹è¯•æ–‡æœ¬: {text}")
        
        start_time = time.perf_counter()
        
        params = {
            "voice_name": "vivienne",
            "text": text,
            "text_lang": "zh",
            "temperature": 1.0,
            "speed_factor": 1.0
        }
        
        response = requests.post(f"{base_url}/tts_with_cached_voice", params=params)
        
        end_time = time.perf_counter()
        duration = end_time - start_time
        
        if response.status_code == 200:
            file_size = len(response.content)
            print(f"âœ… æˆåŠŸ - è€—æ—¶: {duration:.3f}s, æ–‡ä»¶å¤§å°: {file_size/1024:.1f}KB")
        else:
            print(f"âŒ å¤±è´¥ - è€—æ—¶: {duration:.3f}s")

if __name__ == "__main__":
    test_optimization()
```

## ğŸ“Š é¢„æœŸæ€§èƒ½æå‡

### ä¼˜åŒ–å‰ vs ä¼˜åŒ–å

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| çŸ­æ–‡æœ¬(2å­—) | 15.4ç§’ | 4-6ç§’ | 2.5-3.8x |
| ä¸­ç­‰æ–‡æœ¬(20å­—) | 31.5ç§’ | 8-12ç§’ | 2.6-3.9x |
| é•¿æ–‡æœ¬(35å­—) | 45.9ç§’ | 12-18ç§’ | 2.5-3.8x |
| å¹³å‡ååé‡ | 0.04 è¯·æ±‚/ç§’ | 0.15-0.25 è¯·æ±‚/ç§’ | 3.75-6.25x |

### æ€§èƒ½æå‡æ¥æº

1. **CUDA Graph**: å‡å°‘GPUå†…æ ¸å¯åŠ¨å¼€é”€ (20-40%)
2. **åˆ†å—å¤„ç†**: å¹¶è¡Œå¤„ç†éŸ³é¢‘å— (2-4x)
3. **å†…å­˜ä¼˜åŒ–**: å‡å°‘GPUå†…å­˜åˆ†é… (10-20%)

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å…¼å®¹æ€§
- éœ€è¦CUDA 11.0+
- æ”¯æŒCUDA Graphçš„GPU (RTX 20ç³»åˆ—+)
- PyTorch 1.9+

### 2. å†…å­˜è¦æ±‚
- é¢å¤–çš„GPUå†…å­˜ç”¨äºCUDA Graph
- å»ºè®®è‡³å°‘4GBæ˜¾å­˜

### 3. è°ƒè¯•æ¨¡å¼
å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ä¸´æ—¶ç¦ç”¨ä¼˜åŒ–ï¼š
```python
tts_pipeline.use_vocoder_optimization = False
```

### 4. æ€§èƒ½ç›‘æ§
é›†æˆåå¯ä»¥é€šè¿‡æ—¥å¿—è§‚å¯Ÿï¼š
```
ğŸ”„ åˆå§‹åŒ–CUDA Graph (è¾“å…¥å½¢çŠ¶: (1, 100, 500))...
âœ… CUDA Graphåˆå§‹åŒ–å®Œæˆ
âš¡ ä¼˜åŒ–å¤„ç†è€—æ—¶: 2.345s
```

## ğŸ› æ•…éšœæ’é™¤

### Q1: CUDA Graphåˆå§‹åŒ–å¤±è´¥
**åŸå› **: GPUä¸æ”¯æŒæˆ–æ˜¾å­˜ä¸è¶³
**è§£å†³**: æ£€æŸ¥GPUå‹å·å’Œæ˜¾å­˜ï¼Œæˆ–ç¦ç”¨CUDA Graph

### Q2: éŸ³é¢‘è´¨é‡ä¸‹é™
**åŸå› **: åˆ†å—å¤„ç†å¯èƒ½å¼•å…¥è¾¹ç•Œæ•ˆåº”
**è§£å†³**: è°ƒæ•´chunk_sizeæˆ–æ·»åŠ éŸ³é¢‘å¹³æ»‘å¤„ç†

### Q3: å†…å­˜æº¢å‡º
**åŸå› **: åˆ†å—å¤§å°è¿‡å¤§
**è§£å†³**: å‡å°vocoder_chunk_sizeå‚æ•°

## âœ… éªŒè¯æˆåŠŸ

æˆåŠŸé›†æˆåï¼Œåº”è¯¥çœ‹åˆ°ï¼š
1. å¯åŠ¨æ—¶çš„ä¼˜åŒ–åˆå§‹åŒ–æ—¥å¿—
2. å¤„ç†æ—¶é—´æ˜¾è‘—å‡å°‘
3. CUDA Graphåˆå§‹åŒ–ä¿¡æ¯
4. ä¼˜åŒ–å¤„ç†è€—æ—¶æ—¥å¿—

é€šè¿‡ä»¥ä¸Šé›†æˆï¼Œä½ çš„TTSæœåŠ¡æ€§èƒ½å°†å¾—åˆ°æ˜¾è‘—æå‡ï¼
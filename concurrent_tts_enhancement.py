#!/usr/bin/env python3
"""
å¹¶å‘TTSå¢å¼ºæ¨¡å— - æ”¯æŒå¤šGPUå¹¶å‘å¤„ç†
"""
import asyncio
import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Optional, Tuple
import torch
import multiprocessing as mp
from dataclasses import dataclass

@dataclass
class TTSRequest:
    """TTSè¯·æ±‚æ•°æ®ç»“æ„"""
    request_id: str
    text: str
    text_lang: str
    ref_audio_path: str
    prompt_text: str
    prompt_lang: str
    temperature: float
    speed_factor: float
    sample_steps: int
    media_type: str
    other_params: dict

@dataclass
class TTSResponse:
    """TTSå“åº”æ•°æ®ç»“æ„"""
    request_id: str
    success: bool
    audio_data: Optional[bytes]
    error_message: Optional[str]
    processing_time: float
    gpu_id: int

class GPUWorker:
    """å•GPUå·¥ä½œè¿›ç¨‹"""
    
    def __init__(self, gpu_id: int, config_path: str):
        self.gpu_id = gpu_id
        self.config_path = config_path
        self.device = f"cuda:{gpu_id}"
        self.tts_pipeline = None
        self.request_queue = queue.Queue()
        self.response_queue = queue.Queue()
        self.is_running = False
        self.worker_thread = None
        
    def initialize(self):
        """åˆå§‹åŒ–GPUå·¥ä½œå™¨"""
        try:
            # è®¾ç½®å½“å‰GPU
            torch.cuda.set_device(self.gpu_id)
            
            # å¯¼å…¥å¿…è¦æ¨¡å—ï¼ˆé¿å…åœ¨ä¸»è¿›ç¨‹å¯¼å…¥ï¼‰
            import sys
            import os
            sys.path.append(os.getcwd())
            sys.path.append("GPT_SoVITS")
            
            from GPT_SoVITS.TTS_infer_pack.TTS import TTS, TTS_Config
            
            # åˆ›å»ºé…ç½®
            config = TTS_Config(self.config_path)
            config.device = self.device
            
            # åˆ›å»ºTTSå®ä¾‹
            self.tts_pipeline = TTS(config)
            
            # åˆå§‹åŒ–CUDAä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            self._init_gpu_optimization()
            
            print(f"âœ… GPU {self.gpu_id} å·¥ä½œå™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ GPU {self.gpu_id} å·¥ä½œå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _init_gpu_optimization(self):
        """åˆå§‹åŒ–GPUç‰¹å®šçš„ä¼˜åŒ–"""
        try:
            # è®¾ç½®CUDAä¼˜åŒ–
            if hasattr(self.tts_pipeline, 'use_vocoder_optimization'):
                self.tts_pipeline.use_vocoder_optimization = True
                
            # é¢„çƒ­GPU
            dummy_req = {
                "text": "é¢„çƒ­æµ‹è¯•",
                "text_lang": "zh",
                "ref_audio_path": "voice/vivienne/sample.mp3",
                "prompt_text": "Hello",
                "prompt_lang": "en",
                "temperature": 1.0,
                "speed_factor": 1.0,
                "sample_steps": 32
            }
            
            # æ‰§è¡Œé¢„çƒ­
            next(self.tts_pipeline.run(dummy_req))
            print(f"ğŸ”¥ GPU {self.gpu_id} é¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ GPU {self.gpu_id} ä¼˜åŒ–åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def start(self):
        """å¯åŠ¨å·¥ä½œå™¨"""
        if not self.initialize():
            return False
            
        self.is_running = True
        self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self.worker_thread.start()
        return True
    
    def stop(self):
        """åœæ­¢å·¥ä½œå™¨"""
        self.is_running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5)
    
    def _worker_loop(self):
        """å·¥ä½œå™¨ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                # è·å–è¯·æ±‚ï¼ˆè¶…æ—¶æœºåˆ¶ï¼‰
                request = self.request_queue.get(timeout=1.0)
                
                # å¤„ç†è¯·æ±‚
                response = self._process_request(request)
                
                # è¿”å›å“åº”
                self.response_queue.put(response)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ GPU {self.gpu_id} å¤„ç†è¯·æ±‚å¤±è´¥: {e}")
    
    def _process_request(self, request: TTSRequest) -> TTSResponse:
        """å¤„ç†å•ä¸ªTTSè¯·æ±‚"""
        start_time = time.perf_counter()
        
        try:
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¯æ¬¡è¯·æ±‚å‰è®¾ç½®å‚è€ƒéŸ³é¢‘
            if request.ref_audio_path:
                self.tts_pipeline.set_ref_audio(request.ref_audio_path)
                print(f"ğŸ¤ GPU {self.gpu_id} è®¾ç½®å‚è€ƒéŸ³é¢‘: {request.ref_audio_path}")
            
            # æ„å»ºTTSå‚æ•°
            tts_params = {
                "text": request.text,
                "text_lang": request.text_lang,
                "ref_audio_path": request.ref_audio_path,
                "prompt_text": request.prompt_text,
                "prompt_lang": request.prompt_lang,
                "temperature": request.temperature,
                "speed_factor": request.speed_factor,
                "sample_steps": request.sample_steps,
                **request.other_params
            }
            
            # æ‰§è¡ŒTTS
            sr, audio_data = next(self.tts_pipeline.run(tts_params))
            
            # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            import numpy as np
            from io import BytesIO
            import soundfile as sf
            
            audio_buffer = BytesIO()
            sf.write(audio_buffer, audio_data, sr, format=request.media_type)
            audio_bytes = audio_buffer.getvalue()
            
            processing_time = time.perf_counter() - start_time
            
            return TTSResponse(
                request_id=request.request_id,
                success=True,
                audio_data=audio_bytes,
                error_message=None,
                processing_time=processing_time,
                gpu_id=self.gpu_id
            )
            
        except Exception as e:
            processing_time = time.perf_counter() - start_time
            return TTSResponse(
                request_id=request.request_id,
                success=False,
                audio_data=None,
                error_message=str(e),
                processing_time=processing_time,
                gpu_id=self.gpu_id
            )
    
    def submit_request(self, request: TTSRequest) -> bool:
        """æäº¤è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        try:
            self.request_queue.put_nowait(request)
            return True
        except queue.Full:
            return False
    
    def get_response(self) -> Optional[TTSResponse]:
        """è·å–å¤„ç†ç»“æœ"""
        try:
            return self.response_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_queue_size(self) -> Tuple[int, int]:
        """è·å–é˜Ÿåˆ—å¤§å°"""
        return self.request_queue.qsize(), self.response_queue.qsize()

class ConcurrentTTSManager:
    """å¹¶å‘TTSç®¡ç†å™¨"""
    
    def __init__(self, config_path: str, num_gpus: int = 4):
        self.config_path = config_path
        self.num_gpus = num_gpus
        self.workers: List[GPUWorker] = []
        self.response_futures: Dict[str, asyncio.Future] = {}
        self.load_balancer_index = 0
        self.stats = {
            "total_requests": 0,
            "completed_requests": 0,
            "failed_requests": 0,
            "gpu_stats": {}
        }
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰GPUå·¥ä½œå™¨"""
        print(f"ğŸš€ åˆå§‹åŒ– {self.num_gpus} ä¸ªGPUå·¥ä½œå™¨...")
        
        for gpu_id in range(self.num_gpus):
            worker = GPUWorker(gpu_id, self.config_path)
            if worker.start():
                self.workers.append(worker)
                self.stats["gpu_stats"][gpu_id] = {
                    "requests_processed": 0,
                    "total_processing_time": 0,
                    "average_processing_time": 0
                }
                print(f"âœ… GPU {gpu_id} å·¥ä½œå™¨å¯åŠ¨æˆåŠŸ")
            else:
                print(f"âŒ GPU {gpu_id} å·¥ä½œå™¨å¯åŠ¨å¤±è´¥")
        
        if not self.workers:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„GPUå·¥ä½œå™¨")
            return False
            
        # å¯åŠ¨å“åº”å¤„ç†å¾ªç¯
        asyncio.create_task(self._response_handler_loop())
        
        print(f"ğŸ‰ å¹¶å‘TTSç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨GPU: {len(self.workers)}")
        return True
    
    def shutdown(self):
        """å…³é—­æ‰€æœ‰å·¥ä½œå™¨"""
        print("ğŸ”„ å…³é—­æ‰€æœ‰GPUå·¥ä½œå™¨...")
        for worker in self.workers:
            worker.stop()
        self.workers.clear()
    
    async def process_request(self, request: TTSRequest) -> TTSResponse:
        """å¤„ç†TTSè¯·æ±‚ï¼ˆå¼‚æ­¥ï¼‰"""
        # é€‰æ‹©å·¥ä½œå™¨
        worker = self._select_worker()
        if not worker:
            return TTSResponse(
                request_id=request.request_id,
                success=False,
                audio_data=None,
                error_message="æ²¡æœ‰å¯ç”¨çš„GPUå·¥ä½œå™¨",
                processing_time=0,
                gpu_id=-1
            )
        
        # æäº¤è¯·æ±‚
        if not worker.submit_request(request):
            return TTSResponse(
                request_id=request.request_id,
                success=False,
                audio_data=None,
                error_message="å·¥ä½œå™¨é˜Ÿåˆ—å·²æ»¡",
                processing_time=0,
                gpu_id=worker.gpu_id
            )
        
        # åˆ›å»ºFutureç­‰å¾…å“åº”
        future = asyncio.Future()
        self.response_futures[request.request_id] = future
        self.stats["total_requests"] += 1
        
        try:
            # ç­‰å¾…å¤„ç†å®Œæˆ
            response = await future
            
            # æ›´æ–°ç»Ÿè®¡
            if response.success:
                self.stats["completed_requests"] += 1
                gpu_stats = self.stats["gpu_stats"][response.gpu_id]
                gpu_stats["requests_processed"] += 1
                gpu_stats["total_processing_time"] += response.processing_time
                gpu_stats["average_processing_time"] = (
                    gpu_stats["total_processing_time"] / gpu_stats["requests_processed"]
                )
            else:
                self.stats["failed_requests"] += 1
            
            return response
            
        finally:
            # æ¸…ç†Future
            if request.request_id in self.response_futures:
                del self.response_futures[request.request_id]
    
    def _select_worker(self) -> Optional[GPUWorker]:
        """é€‰æ‹©æœ€ä¼˜å·¥ä½œå™¨ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰"""
        if not self.workers:
            return None
        
        # ç®€å•çš„è½®è¯¢è´Ÿè½½å‡è¡¡
        worker = self.workers[self.load_balancer_index]
        self.load_balancer_index = (self.load_balancer_index + 1) % len(self.workers)
        
        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°ï¼Œå¦‚æœå¤ªæ»¡åˆ™å°è¯•ä¸‹ä¸€ä¸ª
        req_queue_size, _ = worker.get_queue_size()
        if req_queue_size > 10:  # é˜Ÿåˆ—å¤ªæ»¡ï¼Œå°è¯•æ‰¾å…¶ä»–GPU
            for _ in range(len(self.workers)):
                worker = self.workers[self.load_balancer_index]
                self.load_balancer_index = (self.load_balancer_index + 1) % len(self.workers)
                req_queue_size, _ = worker.get_queue_size()
                if req_queue_size <= 10:
                    break
        
        return worker
    
    async def _response_handler_loop(self):
        """å“åº”å¤„ç†å¾ªç¯"""
        while True:
            try:
                # æ£€æŸ¥æ‰€æœ‰å·¥ä½œå™¨çš„å“åº”
                for worker in self.workers:
                    response = worker.get_response()
                    if response:
                        # æ‰¾åˆ°å¯¹åº”çš„Futureå¹¶è®¾ç½®ç»“æœ
                        future = self.response_futures.get(response.request_id)
                        if future and not future.done():
                            future.set_result(response)
                
                # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
                await asyncio.sleep(0.01)
                
            except Exception as e:
                print(f"âŒ å“åº”å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(0.1)
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        # æ·»åŠ å®æ—¶é˜Ÿåˆ—çŠ¶æ€
        stats["queue_status"] = {}
        for i, worker in enumerate(self.workers):
            req_size, resp_size = worker.get_queue_size()
            stats["queue_status"][f"gpu_{i}"] = {
                "request_queue": req_size,
                "response_queue": resp_size
            }
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½
        if stats["completed_requests"] > 0:
            total_time = sum(gpu["total_processing_time"] for gpu in stats["gpu_stats"].values())
            stats["overall_average_time"] = total_time / stats["completed_requests"]
            stats["success_rate"] = stats["completed_requests"] / stats["total_requests"] * 100
        else:
            stats["overall_average_time"] = 0
            stats["success_rate"] = 0
        
        return stats

# å…¨å±€å¹¶å‘ç®¡ç†å™¨å®ä¾‹
concurrent_manager: Optional[ConcurrentTTSManager] = None

def initialize_concurrent_tts(config_path: str, num_gpus: int = 4) -> bool:
    """åˆå§‹åŒ–å¹¶å‘TTSç³»ç»Ÿ"""
    global concurrent_manager
    
    try:
        concurrent_manager = ConcurrentTTSManager(config_path, num_gpus)
        return concurrent_manager.initialize()
    except Exception as e:
        print(f"âŒ å¹¶å‘TTSç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def shutdown_concurrent_tts():
    """å…³é—­å¹¶å‘TTSç³»ç»Ÿ"""
    global concurrent_manager
    if concurrent_manager:
        concurrent_manager.shutdown()
        concurrent_manager = None

async def process_concurrent_tts(request_data: dict) -> TTSResponse:
    """å¤„ç†å¹¶å‘TTSè¯·æ±‚"""
    if not concurrent_manager:
        raise RuntimeError("å¹¶å‘TTSç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    # åˆ›å»ºè¯·æ±‚å¯¹è±¡
    request = TTSRequest(
        request_id=f"req_{int(time.time() * 1000000)}",
        text=request_data.get("text", ""),
        text_lang=request_data.get("text_lang", "auto"),
        ref_audio_path=request_data.get("ref_audio_path", ""),
        prompt_text=request_data.get("prompt_text", ""),
        prompt_lang=request_data.get("prompt_lang", "zh"),
        temperature=request_data.get("temperature", 1.0),
        speed_factor=request_data.get("speed_factor", 1.0),
        sample_steps=request_data.get("sample_steps", 32),
        media_type=request_data.get("media_type", "wav"),
        other_params={
            k: v for k, v in request_data.items() 
            if k not in ["text", "text_lang", "ref_audio_path", "prompt_text", 
                        "prompt_lang", "temperature", "speed_factor", "sample_steps", "media_type"]
        }
    )
    
    # å¤„ç†è¯·æ±‚
    return await concurrent_manager.process_request(request)

def get_concurrent_stats() -> dict:
    """è·å–å¹¶å‘ç»Ÿè®¡ä¿¡æ¯"""
    if not concurrent_manager:
        return {"error": "å¹¶å‘TTSç³»ç»Ÿæœªåˆå§‹åŒ–"}
    
    return concurrent_manager.get_stats()

if __name__ == "__main__":
    print("ğŸš€ å¹¶å‘TTSå¢å¼ºæ¨¡å—")
    print("ä¸»è¦åŠŸèƒ½:")
    print("1. å¤šGPUå¹¶å‘å¤„ç†")
    print("2. æ™ºèƒ½è´Ÿè½½å‡è¡¡") 
    print("3. å¼‚æ­¥è¯·æ±‚å¤„ç†")
    print("4. å®æ—¶æ€§èƒ½ç›‘æ§")
    print("\né¢„æœŸæ€§èƒ½æå‡: 4x (4ä¸ªGPUå¹¶å‘)")
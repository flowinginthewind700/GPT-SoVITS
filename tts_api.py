"""
# TTS API with Voice Management

` python tts_api.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/tts_infer.yaml `

## æ‰§è¡Œå‚æ•°:
    `-a` - `ç»‘å®šåœ°å€, é»˜è®¤"127.0.0.1"`
    `-p` - `ç»‘å®šç«¯å£, é»˜è®¤9880`
    `-c` - `TTSé…ç½®æ–‡ä»¶è·¯å¾„, é»˜è®¤"GPT_SoVITS/configs/tts_infer.yaml"`

## è°ƒç”¨:

### æ¨ç†

endpoint: `/tts`
GET:
```
http://127.0.0.1:9880/tts?text=å…ˆå¸åˆ›ä¸šæœªåŠè€Œä¸­é“å´©æ®‚ï¼Œä»Šå¤©ä¸‹ä¸‰åˆ†ï¼Œç›Šå·ç–²å¼Šï¼Œæ­¤è¯šå±æ€¥å­˜äº¡ä¹‹ç§‹ä¹Ÿã€‚&text_lang=zh&ref_audio_path=archive_jingyuan_1.wav&prompt_lang=zh&prompt_text=æˆ‘æ˜¯ã€Œç½—æµ®ã€äº‘éª‘å°†å†›æ™¯å…ƒã€‚ä¸å¿…æ‹˜è°¨ï¼Œã€Œå°†å†›ã€åªæ˜¯ä¸€æ—¶çš„èº«ä»½ï¼Œä½ ç§°å‘¼æˆ‘æ™¯å…ƒä¾¿å¯&text_split_method=cut5&batch_size=1&media_type=wav&streaming_mode=true
```

POST:
```json
{
    "text": "",                   # str.(required) text to be synthesized
    "text_lang: "",               # str.(required) language of the text to be synthesized
    "ref_audio_path": "",         # str.(required) reference audio path
    "aux_ref_audio_paths": [],    # list.(optional) auxiliary reference audio paths for multi-speaker tone fusion
    "prompt_text": "",            # str.(optional) prompt text for the reference audio
    "prompt_lang": "",            # str.(required) language of the prompt text for the reference audio
    "top_k": 5,                   # int. top k sampling
    "top_p": 1,                   # float. top p sampling
    "temperature": 1,             # float. temperature for sampling
    "text_split_method": "cut0",  # str. text split method, see text_segmentation_method.py for details.
    "batch_size": 1,              # int. batch size for inference
    "batch_threshold": 0.75,      # float. threshold for batch splitting.
    "split_bucket": True,         # bool. whether to split the batch into multiple buckets.
    "speed_factor":1.0,           # float. control the speed of the synthesized audio.
    "streaming_mode": False,      # bool. whether to return a streaming response.
    "seed": -1,                   # int. random seed for reproducibility.
    "parallel_infer": True,       # bool. whether to use parallel inference.
    "repetition_penalty": 1.35,   # float. repetition penalty for T2S model.
    "sample_steps": 32,           # int. number of sampling steps for VITS model V3.
    "super_sampling": False       # bool. whether to use super-sampling for audio when using VITS model V3.
}
```

RESP:
æˆåŠŸ: ç›´æ¥è¿”å› wav éŸ³é¢‘æµï¼Œ http code 200
å¤±è´¥: è¿”å›åŒ…å«é”™è¯¯ä¿¡æ¯çš„ json, http code 400

### å‘½ä»¤æ§åˆ¶

endpoint: `/control`

command:
"restart": é‡æ–°è¿è¡Œ
"exit": ç»“æŸè¿è¡Œ

GET:
```
http://127.0.0.1:9880/control?command=restart
```
POST:
```json
{
    "command": "restart"
}
```

RESP: æ— 


### åˆ‡æ¢GPTæ¨¡å‹

endpoint: `/set_gpt_weights`

GET:
```
http://127.0.0.1:9880/set_gpt_weights?weights_path=GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt
```
RESP:
æˆåŠŸ: è¿”å›"success", http code 200
å¤±è´¥: è¿”å›åŒ…å«é”™è¯¯ä¿¡æ¯çš„ json, http code 400


### åˆ‡æ¢Sovitsæ¨¡å‹

endpoint: `/set_sovits_weights`

GET:
```
http://127.0.0.1:9880/set_sovits_weights?weights_path=GPT_SoVITS/pretrained_models/s2G488k.pth
```

RESP:
æˆåŠŸ: è¿”å›"success", http code 200
å¤±è´¥: è¿”å›åŒ…å«é”™è¯¯ä¿¡æ¯çš„ json, http code 400

"""

import os
import sys
import traceback
import json
from typing import Generator

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ€§èƒ½ç›‘æ§
import time
import psutil
import threading
from functools import wraps

# æ£€æŸ¥CUDAä¼˜åŒ–æ¨¡å—å¯ç”¨æ€§
CUDA_OPTIMIZATION_AVAILABLE = True
try:
    import torch
    if not torch.cuda.is_available():
        CUDA_OPTIMIZATION_AVAILABLE = False
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸå§‹Vocoderå¤„ç†")
    else:
        print("âœ… CUDAä¼˜åŒ–ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
except ImportError:
    CUDA_OPTIMIZATION_AVAILABLE = False
    print("âš ï¸ PyTorchä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸå§‹Vocoderå¤„ç†")

# è½»é‡çº§æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def performance_monitor(func_name="", enable_memory_tracking=False):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.perf_counter()
            
            result = func(*args, **kwargs)
            
            end_time = time.perf_counter()
            duration = end_time - start_time
            
            if enable_memory_tracking:
                try:
                    memory_info = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                    print(f"â±ï¸ {func_name or func.__name__}: {duration:.3f}s, å†…å­˜: {memory_info:.1f}MB")
                except:
                    print(f"â±ï¸ {func_name or func.__name__}: {duration:.3f}s")
            else:
                print(f"â±ï¸ {func_name or func.__name__}: {duration:.3f}s")
            
            return result
        return wrapper
    return decorator

# å…¨å±€æ€§èƒ½ç»Ÿè®¡
performance_stats = {
    "total_requests": 0,
    "total_tts_time": 0,
    "total_t2s_time": 0,
    "total_vocoder_time": 0,
    "total_postprocess_time": 0
}

def update_performance_stats(stage, duration):
    """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
    performance_stats["total_requests"] += 1
    if stage == "t2s":
        performance_stats["total_t2s_time"] += duration
    elif stage == "vocoder":
        performance_stats["total_vocoder_time"] += duration
    elif stage == "postprocess":
        performance_stats["total_postprocess_time"] += duration
    elif stage == "total":
        performance_stats["total_tts_time"] += duration


now_dir = os.getcwd()
sys.path.append(now_dir)
sys.path.append("%s/GPT_SoVITS" % (now_dir))

import argparse
import subprocess
import wave
import signal
import numpy as np
import soundfile as sf
from fastapi import FastAPI, Response, File, UploadFile
from fastapi.responses import StreamingResponse, JSONResponse
import uvicorn
from io import BytesIO
from tools.i18n.i18n import I18nAuto
from GPT_SoVITS.TTS_infer_pack.TTS import TTS, TTS_Config
from GPT_SoVITS.TTS_infer_pack.text_segmentation_method import get_method_names as get_cut_method_names
from pydantic import BaseModel

# å¯¼å…¥CUDAä¼˜åŒ–æ¨¡å—
import torch
from typing import List

# å¯¼å…¥å¹¶å‘å¤„ç†æ¨¡å—
try:
    from concurrent_tts_enhancement import (
        initialize_concurrent_tts, 
        shutdown_concurrent_tts,
        process_concurrent_tts,
        get_concurrent_stats
    )
    CONCURRENT_TTS_AVAILABLE = True
    print("âœ… å¹¶å‘TTSæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    CONCURRENT_TTS_AVAILABLE = False
    print(f"âš ï¸ å¹¶å‘TTSæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("å°†ä½¿ç”¨å•çº¿ç¨‹å¤„ç†")

class CUDAGraphVocoder:
    """é«˜æ€§èƒ½CUDA Graphä¼˜åŒ–çš„Vocoder"""
    
    def __init__(self, vocoder_model, device="cuda", max_cached_shapes=3):
        self.vocoder_model = vocoder_model
        self.device = device
        self.max_cached_shapes = max_cached_shapes
        self.cached_graphs = {}  # ç¼“å­˜å¤šä¸ªå½¢çŠ¶çš„graph
        self.warmup_done = False
        
    def initialize_graph(self, input_shape: tuple, num_warmup: int = 3):
        """åˆå§‹åŒ–CUDA Graph - æ”¯æŒå¤šå½¢çŠ¶ç¼“å­˜"""
        if input_shape in self.cached_graphs:
            return True
            
        # é™åˆ¶ç¼“å­˜æ•°é‡ï¼Œæ¸…ç†æœ€è€çš„
        if len(self.cached_graphs) >= self.max_cached_shapes:
            oldest_shape = next(iter(self.cached_graphs))
            del self.cached_graphs[oldest_shape]
            print(f"ğŸ”„ æ¸…ç†æ—§çš„CUDA Graphç¼“å­˜: {oldest_shape}")
            
        print(f"ğŸ”„ åˆå§‹åŒ–CUDA Graph (å½¢çŠ¶: {input_shape})...")
        
        try:
            # å…¨å±€é¢„çƒ­(åªéœ€è¦ä¸€æ¬¡)
            if not self.warmup_done:
                dummy_input = torch.randn(input_shape, device=self.device, dtype=torch.float16)
                for _ in range(num_warmup):
                    with torch.no_grad():
                        _ = self.vocoder_model(dummy_input)
                torch.cuda.synchronize()
                self.warmup_done = True
            
            # åˆ›å»ºé™æ€è¾“å…¥è¾“å‡º
            static_input = torch.randn(input_shape, device=self.device, dtype=torch.float16)
            
            # æ•è·CUDA Graph
            graph = torch.cuda.CUDAGraph()
            with torch.cuda.graph(graph):
                with torch.no_grad():
                    static_output = self.vocoder_model(static_input)
            
            # ç¼“å­˜graphä¿¡æ¯
            self.cached_graphs[input_shape] = {
                'graph': graph,
                'static_input': static_input,
                'static_output': static_output
            }
            
            print(f"âœ… CUDA Graphåˆå§‹åŒ–å®Œæˆ (ç¼“å­˜æ•°: {len(self.cached_graphs)})")
            return True
            
        except Exception as e:
            print(f"âŒ CUDA Graphåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def inference(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨CUDA Graphè¿›è¡Œæ¨ç†"""
        input_shape = tuple(input_tensor.shape)
        
        # æ£€æŸ¥ç¼“å­˜
        if input_shape not in self.cached_graphs:
            if not self.initialize_graph(input_shape):
                # å›é€€åˆ°æ™®é€šæ¨ç†
                with torch.no_grad():
                    return self.vocoder_model(input_tensor)
        
        # ä½¿ç”¨ç¼“å­˜çš„graph
        graph_info = self.cached_graphs[input_shape]
        graph_info['static_input'].copy_(input_tensor)
        graph_info['graph'].replay()
        
        return graph_info['static_output'].clone()

class OptimizedVocoderProcessor:
    """é«˜æ€§èƒ½Vocoderå¤„ç†å™¨"""
    
    def __init__(self, vocoder_model, device="cuda", chunk_size=500, overlap_ratio=0.1):
        self.vocoder_model = vocoder_model
        self.device = device
        self.chunk_size = chunk_size
        self.overlap_ratio = overlap_ratio
        self.cuda_graph_vocoder = CUDAGraphVocoder(vocoder_model, device)
        self.performance_history = []
        
    def get_optimal_chunk_size(self, total_length: int) -> int:
        """æ ¹æ®å†å²æ€§èƒ½æ•°æ®åŠ¨æ€è°ƒæ•´chunk_size"""
        if len(self.performance_history) < 3:
            return self.chunk_size
            
        # ç®€å•çš„è‡ªé€‚åº”ç­–ç•¥
        avg_time_per_chunk = sum(self.performance_history[-3:]) / 3
        if avg_time_per_chunk > 2.0:  # å¦‚æœå¤„ç†æ—¶é—´è¿‡é•¿
            return max(300, self.chunk_size - 100)
        elif avg_time_per_chunk < 0.5:  # å¦‚æœå¤„ç†å¾ˆå¿«
            return min(800, self.chunk_size + 100)
        return self.chunk_size
    
    def split_into_chunks(self, pred_spec: torch.Tensor) -> List[torch.Tensor]:
        """æ™ºèƒ½åˆ†å‰²ä¸ºå—ï¼Œæ”¯æŒé‡å å¤„ç†"""
        total_length = pred_spec.shape[-1]
        optimal_chunk_size = self.get_optimal_chunk_size(total_length)
        
        if total_length <= optimal_chunk_size:
            return [pred_spec]
        
        chunks = []
        overlap_size = int(optimal_chunk_size * self.overlap_ratio)
        step_size = optimal_chunk_size - overlap_size
        
        for start in range(0, total_length, step_size):
            end = min(start + optimal_chunk_size, total_length)
            chunk = pred_spec[:, :, start:end]
            chunks.append(chunk)
            
            if end >= total_length:
                break
        
        return chunks
    
    def merge_overlapped_chunks(self, audio_chunks: List[torch.Tensor]) -> torch.Tensor:
        """åˆå¹¶é‡å çš„éŸ³é¢‘å—ï¼Œä½¿ç”¨çª—å‡½æ•°å¹³æ»‘"""
        if len(audio_chunks) == 1:
            return audio_chunks[0]
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥æ‹¼æ¥ï¼Œå¿½ç•¥é‡å éƒ¨åˆ†çš„å¹³æ»‘å¤„ç†
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å¹³æ»‘ç®—æ³•
        total_length = sum(chunk.shape[-1] for chunk in audio_chunks)
        overlap_size = int(self.chunk_size * self.overlap_ratio)
        adjusted_length = total_length - (len(audio_chunks) - 1) * overlap_size
        
        return torch.cat(audio_chunks, dim=-1)[:, :adjusted_length]
    
    def process_optimized(self, pred_spec: torch.Tensor) -> torch.Tensor:
        """é«˜æ€§èƒ½ä¼˜åŒ–å¤„ç†ä¸»å‡½æ•°"""
        start_time = time.perf_counter()
        
        # åˆ†å‰²ä¸ºå—
        chunks = self.split_into_chunks(pred_spec)
        
        if len(chunks) == 1:
            # å•ä¸ªå—ï¼Œç›´æ¥ä½¿ç”¨CUDA Graph
            result = self.cuda_graph_vocoder.inference(chunks[0])
        else:
            # å¤šä¸ªå—ï¼Œæ‰¹é‡å¤„ç†
            audio_chunks = []
            chunk_start_time = time.perf_counter()
            
            for i, chunk in enumerate(chunks):
                audio_chunk = self.cuda_graph_vocoder.inference(chunk)
                audio_chunks.append(audio_chunk)
                
                # æ¯å¤„ç†5ä¸ªå—è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 5 == 0:
                    progress = (i + 1) / len(chunks) * 100
                    print(f"ğŸ”„ å¤„ç†è¿›åº¦: {progress:.1f}% ({i+1}/{len(chunks)})")
            
            chunk_end_time = time.perf_counter()
            chunk_duration = chunk_end_time - chunk_start_time
            
            # è®°å½•æ€§èƒ½å†å²
            self.performance_history.append(chunk_duration / len(chunks))
            if len(self.performance_history) > 10:
                self.performance_history.pop(0)
            
            # åˆå¹¶å—
            if self.overlap_ratio > 0:
                result = self.merge_overlapped_chunks(audio_chunks)
            else:
                result = torch.cat(audio_chunks, dim=-1)
        
        end_time = time.perf_counter()
        total_duration = end_time - start_time
        
        # è¾“å‡ºè¯¦ç»†æ€§èƒ½ä¿¡æ¯
        input_length = pred_spec.shape[-1]
        processing_speed = input_length / total_duration if total_duration > 0 else 0
        print(f"âš¡ ä¼˜åŒ–å¤„ç†å®Œæˆ: {total_duration:.3f}s, é€Ÿåº¦: {processing_speed:.1f} frames/s, å—æ•°: {len(chunks)}")
        
        return result

# print(sys.path)
i18n = I18nAuto()
cut_method_names = get_cut_method_names()

parser = argparse.ArgumentParser(description="GPT-SoVITS api")
parser.add_argument("-c", "--tts_config", type=str, default="GPT_SoVITS/configs/tts_infer.yaml", help="tts_inferè·¯å¾„")
parser.add_argument("-a", "--bind_addr", type=str, default="127.0.0.1", help="default: 127.0.0.1")
parser.add_argument("-p", "--port", type=int, default="9880", help="default: 9880")
args = parser.parse_args()
config_path = args.tts_config
# device = args.device
port = args.port
host = args.bind_addr
argv = sys.argv

if config_path in [None, ""]:
    config_path = "GPT-SoVITS/configs/tts_infer.yaml"

tts_config = TTS_Config(config_path)
print(tts_config)

# æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶å‘æ¨¡å¼
enable_concurrent = getattr(tts_config, 'enable_concurrent_tts', True)
num_gpus = getattr(tts_config, 'num_gpus', 4)

if False:  # ä¸´æ—¶ç¦ç”¨å¹¶å‘æ¨¡å¼ - ä¿®å¤å£°éŸ³é”™è¯¯é—®é¢˜
    print(f"ğŸš€ å¯ç”¨å¹¶å‘TTSæ¨¡å¼ï¼Œä½¿ç”¨ {num_gpus} ä¸ªGPU")
    CONCURRENT_MODE = True
    # åˆå§‹åŒ–å¹¶å‘ç³»ç»Ÿ
    if not initialize_concurrent_tts(config_path, min(num_gpus, torch.cuda.device_count())):
        print("âš ï¸ å¹¶å‘TTSåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°å•çº¿ç¨‹æ¨¡å¼")
        CONCURRENT_MODE = False
        tts_pipeline = TTS(tts_config)
    else:
        tts_pipeline = None  # å¹¶å‘æ¨¡å¼ä¸‹ä¸éœ€è¦å•ä¸€å®ä¾‹
else:
    print("ğŸ“± ä½¿ç”¨å•çº¿ç¨‹TTSæ¨¡å¼")
    CONCURRENT_MODE = False
    tts_pipeline = TTS(tts_config)

# å•çº¿ç¨‹æ¨¡å¼çš„CUDAä¼˜åŒ–æ”¯æŒ
if not CONCURRENT_MODE and tts_pipeline:
    tts_pipeline.use_vocoder_optimization = getattr(tts_config, 'use_vocoder_optimization', True)
    tts_pipeline.vocoder_chunk_size = getattr(tts_config, 'vocoder_chunk_size', 500)
    tts_pipeline.optimized_processor = None

def init_vocoder_optimization():
    """åˆå§‹åŒ–é«˜æ€§èƒ½Vocoderä¼˜åŒ–"""
    if not tts_pipeline.use_vocoder_optimization or tts_pipeline.optimized_processor is not None:
        return
        
    if not CUDA_OPTIMIZATION_AVAILABLE:
        print("âš ï¸ CUDAä¼˜åŒ–ä¸å¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–åˆå§‹åŒ–")
        tts_pipeline.use_vocoder_optimization = False
        return
        
    try:
        # è·å–vocoderæ¨¡å‹
        vocoder_model = getattr(tts_pipeline, 'vocoder', None)
        if vocoder_model is None:
            print("âš ï¸ æ— æ³•è·å–vocoderæ¨¡å‹ï¼Œè·³è¿‡ä¼˜åŒ–")
            tts_pipeline.use_vocoder_optimization = False
            return
        
        # è·å–é…ç½®å‚æ•°
        chunk_size = getattr(tts_pipeline, 'vocoder_chunk_size', 500)
        overlap_ratio = getattr(tts_config, 'vocoder_overlap_ratio', 0.05)  # 5%é‡å 
        
        print(f"ğŸ”§ åˆå§‹åŒ–ä¼˜åŒ–é…ç½®: chunk_size={chunk_size}, overlap_ratio={overlap_ratio}")
            
        tts_pipeline.optimized_processor = OptimizedVocoderProcessor(
            vocoder_model,
            device=tts_config.device,
            chunk_size=chunk_size,
            overlap_ratio=overlap_ratio
        )
        
        # ä¿å­˜åŸå§‹vocoderæ–¹æ³•
        tts_pipeline._original_vocoder = tts_pipeline.vocoder
        
        # åˆ›å»ºä¼˜åŒ–çš„vocoderæ–¹æ³•
        def optimized_vocoder_call(pred_spec):
            """é«˜æ€§èƒ½ä¼˜åŒ–çš„vocoderè°ƒç”¨"""
            if tts_pipeline.use_vocoder_optimization and tts_pipeline.optimized_processor is not None:
                try:
                    # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
                    if pred_spec.numel() == 0:
                        print("âš ï¸ è¾“å…¥é¢‘è°±ä¸ºç©ºï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•")
                        return tts_pipeline._original_vocoder(pred_spec)
                    
                    return tts_pipeline.optimized_processor.process_optimized(pred_spec)
                except Exception as e:
                    print(f"âš ï¸ ä¼˜åŒ–å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•: {e}")
                    return tts_pipeline._original_vocoder(pred_spec)
            else:
                return tts_pipeline._original_vocoder(pred_spec)
        
        # æ›¿æ¢vocoderæ–¹æ³•
        tts_pipeline.vocoder = optimized_vocoder_call
        
        print("âœ… é«˜æ€§èƒ½Vocoderä¼˜åŒ–åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ Vocoderä¼˜åŒ–åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        tts_pipeline.use_vocoder_optimization = False

APP = FastAPI()

@APP.get("/performance_stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    stats = performance_stats.copy()
    if stats["total_requests"] > 0:
        stats["avg_tts_time"] = stats["total_tts_time"] / stats["total_requests"]
        stats["avg_t2s_time"] = stats["total_t2s_time"] / stats["total_requests"] 
        stats["avg_vocoder_time"] = stats["total_vocoder_time"] / stats["total_requests"]
        stats["avg_postprocess_time"] = stats["total_postprocess_time"] / stats["total_requests"]
        stats["requests_per_second"] = stats["total_requests"] / max(stats["total_tts_time"], 0.001)
    else:
        stats["avg_tts_time"] = 0
        stats["avg_t2s_time"] = 0
        stats["avg_vocoder_time"] = 0
        stats["avg_postprocess_time"] = 0
        stats["requests_per_second"] = 0
    
    # æ·»åŠ ä¼˜åŒ–çŠ¶æ€
    stats["cuda_optimization_enabled"] = getattr(tts_pipeline, 'use_vocoder_optimization', False)
    stats["optimization_initialized"] = getattr(tts_pipeline, 'optimized_processor', None) is not None
    
    return stats

@APP.get("/concurrent_stats")
async def get_concurrent_stats():
    """è·å–å¹¶å‘å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    if CONCURRENT_MODE:
        stats = get_concurrent_stats()
        stats["mode"] = "concurrent"
        stats["gpu_count"] = len(stats.get("gpu_stats", {}))
        return stats
    else:
        return {
            "mode": "single_thread",
            "gpu_count": 1,
            "message": "å½“å‰ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼"
        }

@APP.get("/optimization_status")
async def get_optimization_status():
    """è·å–è¯¦ç»†çš„ä¼˜åŒ–çŠ¶æ€ä¿¡æ¯"""
    status = {
        "cuda_available": CUDA_OPTIMIZATION_AVAILABLE,
        "optimization_enabled": getattr(tts_pipeline, 'use_vocoder_optimization', False),
        "optimization_initialized": getattr(tts_pipeline, 'optimized_processor', None) is not None,
        "config": {
            "chunk_size": getattr(tts_pipeline, 'vocoder_chunk_size', 500),
            "overlap_ratio": getattr(tts_config, 'vocoder_overlap_ratio', 0.05)
        }
    }
    
    # å¦‚æœä¼˜åŒ–å™¨å·²åˆå§‹åŒ–ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
    if status["optimization_initialized"]:
        processor = tts_pipeline.optimized_processor
        cuda_vocoder = processor.cuda_graph_vocoder
        
        status["optimization_details"] = {
            "cached_graph_shapes": len(cuda_vocoder.cached_graphs),
            "warmup_completed": cuda_vocoder.warmup_done,
            "performance_history_length": len(processor.performance_history),
            "avg_chunk_processing_time": sum(processor.performance_history) / len(processor.performance_history) if processor.performance_history else 0
        }
    
    return status

@APP.post("/toggle_optimization")
async def toggle_optimization(enable: bool = True):
    """åŠ¨æ€å¯ç”¨/ç¦ç”¨CUDAä¼˜åŒ–"""
    try:
        if enable:
            tts_pipeline.use_vocoder_optimization = True
            init_vocoder_optimization()
            message = "CUDAä¼˜åŒ–å·²å¯ç”¨"
        else:
            tts_pipeline.use_vocoder_optimization = False
            if hasattr(tts_pipeline, '_original_vocoder'):
                tts_pipeline.vocoder = tts_pipeline._original_vocoder
            message = "CUDAä¼˜åŒ–å·²ç¦ç”¨"
        
        return {"success": True, "message": message}
    except Exception as e:
        return {"success": False, "message": f"æ“ä½œå¤±è´¥: {e}"}


class TTS_Request(BaseModel):
    text: str = None
    text_lang: str = None
    ref_audio_path: str = None
    aux_ref_audio_paths: list = None
    prompt_lang: str = None
    prompt_text: str = ""
    top_k: int = 5
    top_p: float = 1
    temperature: float = 1
    text_split_method: str = "cut5"
    batch_size: int = 1
    batch_threshold: float = 0.75
    split_bucket: bool = False  # v4æ¨¡å‹ä¸æ”¯æŒbucketå¤„ç†ï¼Œé»˜è®¤è®¾ä¸ºFalse
    speed_factor: float = 1.0
    fragment_interval: float = 0.3
    seed: int = -1
    media_type: str = "wav"
    streaming_mode: bool = False
    parallel_infer: bool = True
    repetition_penalty: float = 1.35
    sample_steps: int = 32
    super_sampling: bool = False


### modify from https://github.com/RVC-Boss/GPT-SoVITS/pull/894/files
def pack_ogg(io_buffer: BytesIO, data: np.ndarray, rate: int):
    with sf.SoundFile(io_buffer, mode="w", samplerate=rate, channels=1, format="ogg") as audio_file:
        audio_file.write(data)
    return io_buffer


def pack_raw(io_buffer: BytesIO, data: np.ndarray, rate: int):
    io_buffer.write(data.tobytes())
    return io_buffer


def pack_wav(io_buffer: BytesIO, data: np.ndarray, rate: int):
    io_buffer = BytesIO()
    sf.write(io_buffer, data, rate, format="wav")
    return io_buffer


def pack_aac(io_buffer: BytesIO, data: np.ndarray, rate: int):
    process = subprocess.Popen(
        [
            "ffmpeg",
            "-f",
            "s16le",  # è¾“å…¥16ä½æœ‰ç¬¦å·å°ç«¯æ•´æ•°PCM
            "-ar",
            str(rate),  # è®¾ç½®é‡‡æ ·ç‡
            "-ac",
            "1",  # å•å£°é“
            "-i",
            "pipe:0",  # ä»ç®¡é“è¯»å–è¾“å…¥
            "-c:a",
            "aac",  # éŸ³é¢‘ç¼–ç å™¨ä¸ºAAC
            "-b:a",
            "192k",  # æ¯”ç‰¹ç‡
            "-vn",  # ä¸åŒ…å«è§†é¢‘
            "-f",
            "adts",  # è¾“å‡ºAACæ•°æ®æµæ ¼å¼
            "pipe:1",  # å°†è¾“å‡ºå†™å…¥ç®¡é“
        ],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    out, _ = process.communicate(input=data.tobytes())
    io_buffer.write(out)
    return io_buffer


def pack_audio(io_buffer: BytesIO, data: np.ndarray, rate: int, media_type: str):
    if media_type == "ogg":
        io_buffer = pack_ogg(io_buffer, data, rate)
    elif media_type == "aac":
        io_buffer = pack_aac(io_buffer, data, rate)
    elif media_type == "wav":
        io_buffer = pack_wav(io_buffer, data, rate)
    else:
        io_buffer = pack_raw(io_buffer, data, rate)
    io_buffer.seek(0)
    return io_buffer


# from https://huggingface.co/spaces/coqui/voice-chat-with-mistral/blob/main/app.py
def wave_header_chunk(frame_input=b"", channels=1, sample_width=2, sample_rate=32000):
    # This will create a wave header then append the frame input
    # It should be first on a streaming wav file
    # Other frames better should not have it (else you will hear some artifacts each chunk start)
    wav_buf = BytesIO()
    with wave.open(wav_buf, "wb") as vfout:
        vfout.setnchannels(channels)
        vfout.setsampwidth(sample_width)
        vfout.setframerate(sample_rate)
        vfout.writeframes(frame_input)

    wav_buf.seek(0)
    return wav_buf.read()


def handle_control(command: str):
    if command == "restart":
        os.execl(sys.executable, sys.executable, *argv)
    elif command == "exit":
        os.kill(os.getpid(), signal.SIGTERM)
        exit(0)


def check_params(req: dict):
    text: str = req.get("text", "")
    text_lang: str = req.get("text_lang", "")
    ref_audio_path: str = req.get("ref_audio_path", "")
    streaming_mode: bool = req.get("streaming_mode", False)
    media_type: str = req.get("media_type", "wav")
    prompt_lang: str = req.get("prompt_lang", "")
    text_split_method: str = req.get("text_split_method", "cut5")

    if ref_audio_path in [None, ""]:
        return JSONResponse(status_code=400, content={"message": "ref_audio_path is required"})
    if text in [None, ""]:
        return JSONResponse(status_code=400, content={"message": "text is required"})
    if text_lang in [None, ""]:
        return JSONResponse(status_code=400, content={"message": "text_lang is required"})
    elif text_lang.lower() not in tts_config.languages:
        return JSONResponse(
            status_code=400,
            content={"message": f"text_lang: {text_lang} is not supported in version {tts_config.version}"},
        )
    if prompt_lang in [None, ""]:
        return JSONResponse(status_code=400, content={"message": "prompt_lang is required"})
    elif prompt_lang.lower() not in tts_config.languages:
        return JSONResponse(
            status_code=400,
            content={"message": f"prompt_lang: {prompt_lang} is not supported in version {tts_config.version}"},
        )
    if media_type not in ["wav", "raw", "ogg", "aac"]:
        return JSONResponse(status_code=400, content={"message": f"media_type: {media_type} is not supported"})
    elif media_type == "ogg" and not streaming_mode:
        return JSONResponse(status_code=400, content={"message": "ogg format is not supported in non-streaming mode"})

    if text_split_method not in cut_method_names:
        return JSONResponse(
            status_code=400, content={"message": f"text_split_method:{text_split_method} is not supported"}
        )

    return None


async def tts_handle(req: dict):
    """
    Text to speech handler with concurrent processing support.

    Args:
        req (dict): TTS request parameters
    returns:
        StreamingResponse: audio stream response.
    """
    # æ·»åŠ æ€§èƒ½ç›‘æ§
    total_start_time = time.perf_counter()
    
    try:
        streaming_mode = req.get("streaming_mode", False)
        return_fragment = req.get("return_fragment", False)
        media_type = req.get("media_type", "wav")

        check_res = check_params(req)
        if check_res is not None:
            return check_res

        if streaming_mode or return_fragment:
            req["return_fragment"] = True

        # å¹¶å‘æ¨¡å¼å¤„ç†
        if CONCURRENT_MODE:
            print("ğŸš€ ä½¿ç”¨å¤šGPUå¹¶å‘å¤„ç†")
            return await _handle_concurrent_tts(req, total_start_time)
        
        # å•çº¿ç¨‹æ¨¡å¼å¤„ç†
        return await _handle_single_thread_tts(req, total_start_time)
        
    except Exception as e:
        print(f"âŒ TTSå¤„ç†å¤±è´¥: {e}")
        return JSONResponse(status_code=400, content={"message": "tts failed", "Exception": str(e)})

async def _handle_concurrent_tts(req: dict, total_start_time: float):
    """å¤„ç†å¹¶å‘TTSè¯·æ±‚"""
    try:
        # ä½¿ç”¨å¹¶å‘å¤„ç†
        response = await process_concurrent_tts(req)
        
        if response.success:
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            total_duration = time.perf_counter() - total_start_time
            update_performance_stats("total", total_duration)
            
            print(f"ğŸ¯ å¹¶å‘å¤„ç†å®Œæˆ: {total_duration:.3f}s (GPU {response.gpu_id}, å¤„ç†æ—¶é—´: {response.processing_time:.3f}s)")
            
            media_type = req.get("media_type", "wav")
            return Response(response.audio_data, media_type=f"audio/{media_type}")
        else:
            return JSONResponse(
                status_code=400, 
                content={"message": "concurrent tts failed", "Exception": response.error_message}
            )
            
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "concurrent tts failed", "Exception": str(e)})

async def _handle_single_thread_tts(req: dict, total_start_time: float):
    """å¤„ç†å•çº¿ç¨‹TTSè¯·æ±‚"""
    try:
        # åˆå§‹åŒ–CUDAä¼˜åŒ–
        if tts_pipeline:
            init_vocoder_optimization()
        
        streaming_mode = req.get("streaming_mode", False)
        media_type = req.get("media_type", "wav")

        # å¦‚æœå¯ç”¨äº†ä¼˜åŒ–ï¼Œæ˜¾ç¤ºæç¤º
        if hasattr(tts_pipeline, 'use_vocoder_optimization') and tts_pipeline.use_vocoder_optimization:
            print("ğŸš€ ä½¿ç”¨å•GPU CUDAä¼˜åŒ–å¤„ç†")

        # T2Sæ¨ç†é˜¶æ®µ
        t2s_start_time = time.perf_counter()
        tts_generator = tts_pipeline.run(req)
        t2s_end_time = time.perf_counter()
        t2s_duration = t2s_end_time - t2s_start_time
        update_performance_stats("t2s", t2s_duration)
        print(f"â±ï¸ T2Sæ¨ç†è€—æ—¶: {t2s_duration:.3f}s")

        # Vocoderåˆæˆå’ŒéŸ³é¢‘åå¤„ç†é˜¶æ®µ
        vocoder_start_time = time.perf_counter()

        if streaming_mode:
            def streaming_generator(tts_generator: Generator, media_type: str):
                if_frist_chunk = True
                for sr, chunk in tts_generator:
                    if if_frist_chunk and media_type == "wav":
                        yield wave_header_chunk(sample_rate=sr)
                        media_type = "raw"
                        if_frist_chunk = False
                    yield pack_audio(BytesIO(), chunk, sr, media_type).getvalue()

            vocoder_end_time = time.perf_counter()
            vocoder_duration = vocoder_end_time - vocoder_start_time
            update_performance_stats("vocoder", vocoder_duration)
            print(f"â±ï¸ Vocoderåˆæˆè€—æ—¶: {vocoder_duration:.3f}s")

            # æ€»è€—æ—¶
            total_end_time = time.perf_counter()
            total_duration = total_end_time - total_start_time
            update_performance_stats("total", total_duration)
            print(f"ğŸ¯ æ€»è€—æ—¶: {total_duration:.3f}s (T2S: {t2s_duration:.3f}s, Vocoder: {vocoder_duration:.3f}s)")

            return StreamingResponse(
                streaming_generator(tts_generator, media_type),
                media_type=f"audio/{media_type}",
            )
        else:
            # éæµå¼æ¨¡å¼
            sr, audio_data = next(tts_generator)
            
            vocoder_end_time = time.perf_counter()
            vocoder_duration = vocoder_end_time - vocoder_start_time
            update_performance_stats("vocoder", vocoder_duration)
            print(f"â±ï¸ Vocoderåˆæˆè€—æ—¶: {vocoder_duration:.3f}s")
            
            # éŸ³é¢‘åå¤„ç†é˜¶æ®µ
            postprocess_start_time = time.perf_counter()
            audio_data = pack_audio(BytesIO(), audio_data, sr, media_type).getvalue()
            postprocess_end_time = time.perf_counter()
            postprocess_duration = postprocess_end_time - postprocess_start_time
            update_performance_stats("postprocess", postprocess_duration)
            print(f"â±ï¸ éŸ³é¢‘åå¤„ç†è€—æ—¶: {postprocess_duration:.3f}s")
            
            # æ€»è€—æ—¶
            total_end_time = time.perf_counter()
            total_duration = total_end_time - total_start_time
            update_performance_stats("total", total_duration)
            print(f"ğŸ¯ æ€»è€—æ—¶: {total_duration:.3f}s (T2S: {t2s_duration:.3f}s, Vocoder: {vocoder_duration:.3f}s, åå¤„ç†: {postprocess_duration:.3f}s)")
            
            return Response(audio_data, media_type=f"audio/{media_type}")
        
    except Exception as e:
        print(f"âŒ å•çº¿ç¨‹TTSå¤„ç†å¤±è´¥: {e}")
        return JSONResponse(status_code=400, content={"message": "single thread tts failed", "Exception": str(e)})


@APP.get("/control")
async def control(command: str = None):
    if command is None:
        return JSONResponse(status_code=400, content={"message": "command is required"})
    handle_control(command)


@APP.get("/tts")
async def tts_get_endpoint(
    text: str = None,
    text_lang: str = None,
    ref_audio_path: str = None,
    aux_ref_audio_paths: list = None,
    prompt_lang: str = None,
    prompt_text: str = "",
    top_k: int = 5,
    top_p: float = 1,
    temperature: float = 1,
    text_split_method: str = "cut0",
    batch_size: int = 1,
    batch_threshold: float = 0.75,
    split_bucket: bool = False,  # v4æ¨¡å‹ä¸æ”¯æŒbucketå¤„ç†
    speed_factor: float = 1.0,
    fragment_interval: float = 0.3,
    seed: int = -1,
    media_type: str = "wav",
    streaming_mode: bool = False,
    parallel_infer: bool = True,
    repetition_penalty: float = 1.35,
    sample_steps: int = 32,
    super_sampling: bool = False,
):
    req = {
        "text": text,
        "text_lang": text_lang.lower(),
        "ref_audio_path": ref_audio_path,
        "aux_ref_audio_paths": aux_ref_audio_paths,
        "prompt_text": prompt_text,
        "prompt_lang": prompt_lang.lower(),
        "top_k": top_k,
        "top_p": top_p,
        "temperature": temperature,
        "text_split_method": text_split_method,
        "batch_size": int(batch_size),
        "batch_threshold": float(batch_threshold),
        "speed_factor": float(speed_factor),
        "split_bucket": split_bucket,
        "fragment_interval": fragment_interval,
        "seed": seed,
        "media_type": media_type,
        "streaming_mode": streaming_mode,
        "parallel_infer": parallel_infer,
        "repetition_penalty": float(repetition_penalty),
        "sample_steps": int(sample_steps),
        "super_sampling": super_sampling,
    }
    return await tts_handle(req)


@APP.post("/tts")
async def tts_post_endpoint(request: TTS_Request):
    req = request.dict()
    return await tts_handle(req)


@APP.get("/set_refer_audio")
async def set_refer_aduio(refer_audio_path: str = None):
    try:
        tts_pipeline.set_ref_audio(refer_audio_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "set refer audio failed", "Exception": str(e)})
    return JSONResponse(status_code=200, content={"message": "success"})


@APP.post("/upload_voice")
async def upload_voice(
    voice_name: str,
    audio_file: UploadFile = File(...),
    text_file: UploadFile = File(None),
    gender: str = "unknown",
    description: str = "",
    language: str = "zh"
):
    """ä¸Šä¼ æ–°çš„éŸ³è‰²"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not audio_file.content_type.startswith("audio/"):
            return JSONResponse(status_code=400, content={"message": "Audio file type not supported"})
        
        # åˆ›å»ºvoiceç›®å½•
        voice_path = os.path.join("voice", voice_name)
        os.makedirs(voice_path, exist_ok=True)
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        audio_filename = f"sample.{audio_file.filename.split('.')[-1]}"
        audio_path = os.path.join(voice_path, audio_filename)
        with open(audio_path, "wb") as f:
            f.write(await audio_file.read())
        
        # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
        text_filename = None
        if text_file:
            text_filename = "sample.wav.txt"
            text_path = os.path.join(voice_path, text_filename)
            with open(text_path, "wb") as f:
                f.write(await text_file.read())
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config = {
            "name": voice_name,
            "gender": gender,
            "description": description,
            "language": language
        }
        config_path = os.path.join(voice_path, "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # åˆ·æ–°ç¼“å­˜
        success, message = load_voice_to_cache(voice_name)
        
        return JSONResponse(status_code=200, content={
            "message": "success",
            "voice_name": voice_name,
            "audio_file": audio_filename,
            "text_file": text_filename,
            "cache_loaded": success
        })
        
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to upload voice", "Exception": str(e)})

@APP.delete("/voice/{voice_name}")
async def delete_voice(voice_name: str):
    """åˆ é™¤éŸ³è‰²"""
    try:
        voice_path = os.path.join("voice", voice_name)
        if not os.path.exists(voice_path):
            return JSONResponse(status_code=404, content={"message": f"Voice '{voice_name}' not found"})
        
        # ä»ç¼“å­˜ä¸­ç§»é™¤
        if voice_name in voice_cache:
            del voice_cache[voice_name]
        
        # åˆ é™¤ç›®å½•
        import shutil
        shutil.rmtree(voice_path)
        
        return JSONResponse(status_code=200, content={"message": f"Voice '{voice_name}' deleted successfully"})
        
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to delete voice", "Exception": str(e)})

@APP.post("/set_refer_audio")
async def set_refer_aduio_post(audio_file: UploadFile = File(...)):
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼Œç¡®ä¿æ˜¯éŸ³é¢‘æ–‡ä»¶
        if not audio_file.content_type.startswith("audio/"):
            return JSONResponse(status_code=400, content={"message": "file type is not supported"})

        os.makedirs("uploaded_audio", exist_ok=True)
        save_path = os.path.join("uploaded_audio", audio_file.filename)
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°æœåŠ¡å™¨ä¸Šçš„ä¸€ä¸ªç›®å½•
        with open(save_path , "wb") as buffer:
            buffer.write(await audio_file.read())

        tts_pipeline.set_ref_audio(save_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": f"set refer audio failed", "Exception": str(e)})
    return JSONResponse(status_code=200, content={"message": "success"})


@APP.get("/set_gpt_weights")
async def set_gpt_weights(weights_path: str = None):
    try:
        if weights_path in ["", None]:
            return JSONResponse(status_code=400, content={"message": "gpt weight path is required"})
        tts_pipeline.init_t2s_weights(weights_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "change gpt weight failed", "Exception": str(e)})

    return JSONResponse(status_code=200, content={"message": "success"})


@APP.get("/set_sovits_weights")
async def set_sovits_weights(weights_path: str = None):
    try:
        if weights_path in ["", None]:
            return JSONResponse(status_code=400, content={"message": "sovits weight path is required"})
        tts_pipeline.init_vits_weights(weights_path)
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "change sovits weight failed", "Exception": str(e)})
    return JSONResponse(status_code=200, content={"message": "success"})


# Voice Cache Management
voice_cache = {}  # å…¨å±€éŸ³è‰²ç¼“å­˜

def load_voice_to_cache(voice_name: str):
    """å°†éŸ³è‰²åŠ è½½åˆ°å†…å­˜ç¼“å­˜"""
    try:
        voice_info = get_voice_info(voice_name)
        if not voice_info:
            return False, f"Voice '{voice_name}' not found"
        
        voice_path = os.path.join("voice", voice_name)
        
        # è·å–éŸ³é¢‘æ–‡ä»¶
        if not voice_info["audio_files"]:
            return False, f"No audio file found for voice '{voice_name}'"
        
        audio_file = voice_info["audio_files"][0]
        audio_path = os.path.join(voice_path, audio_file)
        
        if not os.path.exists(audio_path):
            return False, f"Audio file '{audio_path}' not found"
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶åˆ°å†…å­˜
        with open(audio_path, 'rb') as f:
            audio_data = f.read()
        
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        possible_text_files = [
            audio_file.rsplit('.', 1)[0] + '.txt',  # sample.txt
            audio_file.rsplit('.', 1)[0] + '.txt',  # sample.wav.txt
            'sample.txt',  # ç›´æ¥ä½¿ç”¨sample.txt
            'sample.wav.txt'  # ç›´æ¥ä½¿ç”¨sample.wav.txt
        ]
        
        prompt_text = ""
        for text_file in possible_text_files:
            text_path = os.path.join(voice_path, text_file)
            if os.path.exists(text_path):
                try:
                    with open(text_path, 'r', encoding='utf-8') as f:
                        prompt_text = f.read().strip()
                    break
                except:
                    continue
        
        # ç¼“å­˜éŸ³è‰²ä¿¡æ¯
        voice_cache[voice_name] = {
            "audio_data": audio_data,
            "audio_path": audio_path,
            "prompt_text": prompt_text,
            "gender": voice_info.get("gender", "unknown"),
            "description": voice_info.get("description", ""),
            "language": voice_info.get("language", "zh"),
            "audio_filename": audio_file,
            "text_filename": text_file if prompt_text else None
        }
        
        return True, f"Voice '{voice_name}' loaded to cache successfully"
        
    except Exception as e:
        return False, f"Failed to load voice '{voice_name}': {str(e)}"

def refresh_voice_cache():
    """åˆ·æ–°æ‰€æœ‰éŸ³è‰²ç¼“å­˜"""
    global voice_cache
    voice_cache.clear()
    
    voices = scan_voice_directory()
    success_count = 0
    error_count = 0
    errors = []
    
    for voice_name in voices.keys():
        success, message = load_voice_to_cache(voice_name)
        if success:
            success_count += 1
        else:
            error_count += 1
            errors.append(f"{voice_name}: {message}")
    
    return {
        "success_count": success_count,
        "error_count": error_count,
        "total_voices": len(voices),
        "errors": errors
    }

def get_cached_voice(voice_name: str):
    """è·å–ç¼“å­˜çš„éŸ³è‰²ä¿¡æ¯"""
    return voice_cache.get(voice_name)

def set_cached_voice_reference(voice_name: str):
    """ä½¿ç”¨ç¼“å­˜çš„éŸ³è‰²è®¾ç½®å‚è€ƒéŸ³é¢‘ï¼Œå¦‚æœç¼“å­˜ä¸­æ²¡æœ‰åˆ™è‡ªåŠ¨åŠ è½½"""
    cached_voice = get_cached_voice(voice_name)
    if not cached_voice:
        # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ç£ç›˜åŠ è½½
        print(f"ğŸ”„ ç¼“å­˜ä¸­æœªæ‰¾åˆ°éŸ³è‰² '{voice_name}'ï¼Œæ­£åœ¨ä»ç£ç›˜åŠ è½½...")
        success, message = load_voice_to_cache(voice_name)
        if not success:
            raise ValueError(f"Voice '{voice_name}' not found in cache and failed to load from disk: {message}")
        
        # é‡æ–°è·å–ç¼“å­˜
        cached_voice = get_cached_voice(voice_name)
        if not cached_voice:
            raise ValueError(f"Voice '{voice_name}' failed to load into cache")
        
        print(f"âœ… éŸ³è‰² '{voice_name}' å·²åŠ è½½åˆ°ç¼“å­˜")
    
    # ğŸ”§ ä¿®å¤ï¼šåœ¨å¹¶å‘æ¨¡å¼ä¸‹ä¸éœ€è¦è®¾ç½®å…¨å±€pipelineï¼ŒéŸ³é¢‘è·¯å¾„ä¼šä¼ é€’ç»™å„ä¸ªGPUå·¥ä½œå™¨
    if not CONCURRENT_MODE and tts_pipeline:
        # å•çº¿ç¨‹æ¨¡å¼æ‰è®¾ç½®å‚è€ƒéŸ³é¢‘
        tts_pipeline.set_ref_audio(cached_voice["audio_path"])
        print(f"ğŸ¤ å•çº¿ç¨‹æ¨¡å¼è®¾ç½®å‚è€ƒéŸ³é¢‘: {cached_voice['audio_path']}")
    elif CONCURRENT_MODE:
        print(f"ğŸ¤ å¹¶å‘æ¨¡å¼ï¼ŒéŸ³é¢‘è·¯å¾„å°†ä¼ é€’ç»™GPUå·¥ä½œå™¨: {cached_voice['audio_path']}")
    
    return cached_voice

# Voice Management Functions
def scan_voice_directory():
    """æ‰«ævoiceç›®å½•ï¼Œè·å–æ‰€æœ‰å¯ç”¨çš„voice"""
    voice_dir = "voice"
    voices = {}
    
    if not os.path.exists(voice_dir):
        return voices
    
    for voice_name in os.listdir(voice_dir):
        voice_path = os.path.join(voice_dir, voice_name)
        if os.path.isdir(voice_path):
            voice_info = {
                "name": voice_name,
                "audio_files": [],
                "text_files": [],
                "gender": "unknown"
            }
            
            # æ‰«æéŸ³é¢‘æ–‡ä»¶
            for file in os.listdir(voice_path):
                file_path = os.path.join(voice_path, file)
                if file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):
                    voice_info["audio_files"].append(file)
                elif file.lower().endswith('.txt'):
                    voice_info["text_files"].append(file)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶
            config_file = os.path.join(voice_path, "config.json")
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        voice_info.update(config)
                except:
                    pass
            
            voices[voice_name] = voice_info
    
    return voices


def get_voice_info(voice_name: str):
    """è·å–æŒ‡å®švoiceçš„ä¿¡æ¯"""
    voices = scan_voice_directory()
    return voices.get(voice_name)


def set_voice_reference(voice_name: str, audio_file: str = None):
    """è®¾ç½®voiceçš„å‚è€ƒéŸ³é¢‘"""
    voice_info = get_voice_info(voice_name)
    if not voice_info:
        raise ValueError(f"Voice '{voice_name}' not found")
    
    voice_path = os.path.join("voice", voice_name)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šéŸ³é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
    if not audio_file and voice_info["audio_files"]:
        audio_file = voice_info["audio_files"][0]
    
    if not audio_file:
        raise ValueError(f"No audio file found for voice '{voice_name}'")
    
    audio_path = os.path.join(voice_path, audio_file)
    if not os.path.exists(audio_path):
        raise ValueError(f"Audio file '{audio_path}' not found")
    
    # ğŸ”§ ä¿®å¤ï¼šåœ¨å¹¶å‘æ¨¡å¼ä¸‹ä¸éœ€è¦è®¾ç½®å…¨å±€pipeline
    if not CONCURRENT_MODE and tts_pipeline:
        # å•çº¿ç¨‹æ¨¡å¼æ‰è®¾ç½®å‚è€ƒéŸ³é¢‘
        tts_pipeline.set_ref_audio(audio_path)
        print(f"ğŸ¤ å•çº¿ç¨‹æ¨¡å¼è®¾ç½®å‚è€ƒéŸ³é¢‘: {audio_path}")
    elif CONCURRENT_MODE:
        print(f"ğŸ¤ å¹¶å‘æ¨¡å¼ï¼ŒéŸ³é¢‘è·¯å¾„å°†ä¼ é€’ç»™GPUå·¥ä½œå™¨: {audio_path}")
    
    # å¦‚æœæœ‰å¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶ï¼Œä¹Ÿè®¾ç½®prompt_text
    # å°è¯•å¤šç§å¯èƒ½çš„æ–‡æœ¬æ–‡ä»¶å
    possible_text_files = [
        audio_file.rsplit('.', 1)[0] + '.txt',  # sample.txt
        audio_file.rsplit('.', 1)[0] + '.txt',  # sample.wav.txt
        'sample.txt',  # ç›´æ¥ä½¿ç”¨sample.txt
        'sample.wav.txt'  # ç›´æ¥ä½¿ç”¨sample.wav.txt
    ]
    
    prompt_text = ""
    for text_file in possible_text_files:
        text_path = os.path.join(voice_path, text_file)
        if os.path.exists(text_path):
            try:
                with open(text_path, 'r', encoding='utf-8') as f:
                    prompt_text = f.read().strip()
                print(f"Debug: æ‰¾åˆ°æ–‡æœ¬æ–‡ä»¶ {text_path}, å†…å®¹: {prompt_text}")
                break
            except Exception as e:
                print(f"Debug: è¯»å–æ–‡æœ¬æ–‡ä»¶ {text_path} å¤±è´¥: {e}")
                continue
    
    return {
        "audio_path": audio_path,
        "prompt_text": prompt_text,
        "gender": voice_info.get("gender", "unknown")
    }


# Voice Management Endpoints
@APP.get("/voices")
async def list_voices():
    """è·å–æ‰€æœ‰å¯ç”¨çš„voiceåˆ—è¡¨"""
    try:
        voices = scan_voice_directory()
        return JSONResponse(status_code=200, content={"voices": voices})
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to list voices", "Exception": str(e)})


@APP.get("/voice/{voice_name}")
async def get_voice(voice_name: str):
    """è·å–æŒ‡å®švoiceçš„è¯¦ç»†ä¿¡æ¯"""
    try:
        voice_info = get_voice_info(voice_name)
        if not voice_info:
            return JSONResponse(status_code=404, content={"message": f"Voice '{voice_name}' not found"})
        return JSONResponse(status_code=200, content={"voice": voice_info})
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to get voice", "Exception": str(e)})


@APP.post("/voice/{voice_name}/set")
async def set_voice(voice_name: str, audio_file: str = None):
    """è®¾ç½®æŒ‡å®šçš„voiceä¸ºå½“å‰å‚è€ƒéŸ³é¢‘"""
    try:
        voice_ref = set_voice_reference(voice_name, audio_file)
        return JSONResponse(status_code=200, content={
            "message": "success",
            "voice_name": voice_name,
            "audio_path": voice_ref["audio_path"],
            "prompt_text": voice_ref["prompt_text"],
            "gender": voice_ref["gender"]
        })
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to set voice", "Exception": str(e)})


@APP.post("/refresh_voice_cache")
async def refresh_voice_cache_endpoint():
    """åˆ·æ–°éŸ³è‰²ç¼“å­˜"""
    try:
        result = refresh_voice_cache()
        return JSONResponse(status_code=200, content={
            "message": "success",
            "result": result
        })
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to refresh voice cache", "Exception": str(e)})

@APP.get("/performance_stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        avg_tts_time = performance_stats["total_tts_time"] / max(performance_stats["total_requests"], 1)
        avg_t2s_time = performance_stats["total_t2s_time"] / max(performance_stats["total_requests"], 1)
        avg_vocoder_time = performance_stats["total_vocoder_time"] / max(performance_stats["total_requests"], 1)
        avg_postprocess_time = performance_stats["total_postprocess_time"] / max(performance_stats["total_requests"], 1)
        
        return JSONResponse(status_code=200, content={
            "total_requests": performance_stats["total_requests"],
            "average_times": {
                "total_tts": round(avg_tts_time, 3),
                "t2s_inference": round(avg_t2s_time, 3),
                "vocoder_synthesis": round(avg_vocoder_time, 3),
                "audio_postprocess": round(avg_postprocess_time, 3)
            },
            "total_times": {
                "total_tts": round(performance_stats["total_tts_time"], 3),
                "t2s_inference": round(performance_stats["total_t2s_time"], 3),
                "vocoder_synthesis": round(performance_stats["total_vocoder_time"], 3),
                "audio_postprocess": round(performance_stats["total_postprocess_time"], 3)
            }
        })
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to get performance stats", "Exception": str(e)})

@APP.post("/reset_performance_stats")
async def reset_performance_stats():
    """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
    try:
        global performance_stats
        performance_stats = {
            "total_requests": 0,
            "total_tts_time": 0,
            "total_t2s_time": 0,
            "total_vocoder_time": 0,
            "total_postprocess_time": 0
        }
        return JSONResponse(status_code=200, content={"message": "Performance stats reset successfully"})
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to reset performance stats", "Exception": str(e)})

@APP.get("/voice_cache_status")
async def get_voice_cache_status():
    """è·å–éŸ³è‰²ç¼“å­˜çŠ¶æ€"""
    try:
        cached_voices = list(voice_cache.keys())
        return JSONResponse(status_code=200, content={
            "cached_voices": cached_voices,
            "cache_size": len(voice_cache),
            "cache_info": {name: {
                "audio_size": len(info["audio_data"]),
                "prompt_text": info["prompt_text"][:50] + "..." if len(info["prompt_text"]) > 50 else info["prompt_text"],
                "gender": info["gender"]
            } for name, info in voice_cache.items()}
        })
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "Failed to get cache status", "Exception": str(e)})

@APP.post("/tts_with_cached_voice")
async def tts_with_cached_voice(
    voice_name: str,
    text: str,
    text_lang: str = "zh",
    top_k: int = 5,
    top_p: float = 1.0,
    temperature: float = 1.0,
    text_split_method: str = "cut5",
    batch_size: int = 1,
    batch_threshold: float = 0.75,
    split_bucket: bool = False,
    speed_factor: float = 1.0,
    fragment_interval: float = 0.3,
    seed: int = -1,
    media_type: str = "wav",
    streaming_mode: bool = False,
    parallel_infer: bool = True,
    repetition_penalty: float = 1.35,
    sample_steps: int = 32,
    super_sampling: bool = False
):
    """ä½¿ç”¨ç¼“å­˜çš„éŸ³è‰²è¿›è¡ŒTTSæ¨ç†"""
    try:
        # ä½¿ç”¨ç¼“å­˜çš„éŸ³è‰²
        cached_voice = set_cached_voice_reference(voice_name)
        
        # æ„å»ºTTSè¯·æ±‚
        req = {
            "text": text,
            "text_lang": text_lang.lower(),
            "ref_audio_path": cached_voice["audio_path"],
            "aux_ref_audio_paths": [],
            "prompt_text": cached_voice["prompt_text"],
            "prompt_lang": text_lang.lower(),
            "top_k": top_k,
            "top_p": top_p,
            "temperature": temperature,
            "text_split_method": text_split_method,
            "batch_size": batch_size,
            "batch_threshold": batch_threshold,
            "split_bucket": split_bucket,
            "speed_factor": speed_factor,
            "fragment_interval": fragment_interval,
            "seed": seed,
            "media_type": media_type,
            "streaming_mode": streaming_mode,
            "parallel_infer": parallel_infer,
            "repetition_penalty": repetition_penalty,
            "sample_steps": sample_steps,
            "super_sampling": super_sampling,
        }
        
        return await tts_handle(req)
        
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "TTS with cached voice failed", "Exception": str(e)})

@APP.post("/tts_with_voice")
async def tts_with_voice(
    voice_name: str,
    text: str,
    text_lang: str = "zh",
    audio_file: str = None,
    top_k: int = 5,
    top_p: float = 1.0,
    temperature: float = 1.0,
    text_split_method: str = "cut5",
    batch_size: int = 1,
    batch_threshold: float = 0.75,
    split_bucket: bool = False,
    speed_factor: float = 1.0,
    fragment_interval: float = 0.3,
    seed: int = -1,
    media_type: str = "wav",
    streaming_mode: bool = False,
    parallel_infer: bool = True,
    repetition_penalty: float = 1.35,
    sample_steps: int = 32,
    super_sampling: bool = False
):
    """ä½¿ç”¨æŒ‡å®švoiceè¿›è¡ŒTTSæ¨ç†"""
    try:
        # è®¾ç½®voice
        voice_ref = set_voice_reference(voice_name, audio_file)
        
        # æ„å»ºTTSè¯·æ±‚
        req = {
            "text": text,
            "text_lang": text_lang.lower(),
            "ref_audio_path": voice_ref["audio_path"],
            "aux_ref_audio_paths": [],
            "prompt_text": voice_ref["prompt_text"],
            "prompt_lang": text_lang.lower(),
            "top_k": top_k,
            "top_p": top_p,
            "temperature": temperature,
            "text_split_method": text_split_method,
            "batch_size": batch_size,
            "batch_threshold": batch_threshold,
            "split_bucket": split_bucket,
            "speed_factor": speed_factor,
            "fragment_interval": fragment_interval,
            "seed": seed,
            "media_type": media_type,
            "streaming_mode": streaming_mode,
            "parallel_infer": parallel_infer,
            "repetition_penalty": repetition_penalty,
            "sample_steps": sample_steps,
            "super_sampling": super_sampling,
        }
        
        return await tts_handle(req)
        
    except Exception as e:
        return JSONResponse(status_code=400, content={"message": "TTS with voice failed", "Exception": str(e)})


if __name__ == "__main__":
    try:
        if host == "None":  # åœ¨è°ƒç”¨æ—¶ä½¿ç”¨ -a None å‚æ•°ï¼Œå¯ä»¥è®©apiç›‘å¬åŒæ ˆ
            host = None
        uvicorn.run(app=APP, host=host, port=port, workers=1)
    except Exception:
        traceback.print_exc()
        os.kill(os.getpid(), signal.SIGTERM)
        exit(0)
